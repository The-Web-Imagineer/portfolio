# AI Photo Verbalizer

<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
![verbalize-photos-screenshot](/assets/verbalize-photos-ss.png)
=======
![verbalize-photos-screenshot](/images/verbalize-photos-ss.png)
>>>>>>> ab0c7ff (fix links)
=======
![verbalize-photos-screenshot](/assets/verbalize-photos-ss.png)
>>>>>>> fe6549c (cleanup)
=======
![verbalize-photos-screenshot](/assets/verbalize-photos-ss.png)
>>>>>>> 79b4943 (changes)

Try it out: [verbalize.photos](verbalize.photos)

The majority of the internet is _heavily_ visual. Over _3 billion_ images are shared over the internet _daily_, most of which are completely hidden from users who can’t physically see them. Developers do not do enough - skimping on providing descriptive alt tags, aria-labels, and other means of supporting the accessibility of visual content. This project’s goal was to expand the experience of visual content by involving other senses in the absence of sight.

I built this tool as the final project for my _Human Computer Interaction_ course at NYU. It aims to improve the experience of the visual side of the internet for visually-impaired users. Users can simply paste a photo or a link to a photo, and a human-like verbal description of the photo will be generated using Computer Vision, Natural Language Processing (NLP), and Speech Synthesis, along with a braille transcription (for Refreshable Braille Displays (RBDs)), and relevant “ambience” to further enhance the visual experience by involving another sense of perception (as well the user’s imagination), to better synthesize a visually-compensatory experience. Supports _voice control_, _keyboard navigation_, and _multiple languages_ (English, French, Polish, Russian, Ukrainian).

## Skills

- TypeScript
- React
- Node.js
- AWS
- Azure
- Firebase
- Material UI
- SCSS
